# -*- coding: utf-8 -*-
"""Conversion_Gpu_to_Cpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LnFTcJLyAMUyfgsKrA4TSoCkHQL9XWMK
"""

!pip install onnx
!pip install onnxruntime
import time
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import onnx
import onnxruntime as ort

class ModelConverter:
    def __init__(self, gpu_model_path):
        self.gpu_model = torch.load(gpu_model_path)

    def convert_to_onnx(self, onnx_path):
        input_names = ["input"]
        output_names = ["output"]
        dummy_input = torch.randn(1, 3, 224, 224)  # Adjust size as needed

        torch.onnx.export(self.gpu_model, dummy_input, onnx_path,
                          input_names=input_names, output_names=output_names)
        print(f"Model converted to ONNX and saved at {onnx_path}")

    def optimize_onnx_model(self, onnx_path, optimized_path):
        model = onnx.load(onnx_path)
        optimized_model = onnx.optimizer.optimize(model)
        onnx.save(optimized_model, optimized_path)
        print(f"Optimized ONNX model saved at {optimized_path}")

    def quantize_model(self, onnx_path, quantized_path):
        from onnxruntime.quantization import quantize_dynamic
        quantize_dynamic(onnx_path, quantized_path)
        print(f"Quantized model saved at {quantized_path}")

class CPUOptimizer:
    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)

    def set_intra_op_num_threads(self, num_threads):
        options = ort.SessionOptions()
        options.intra_op_num_threads = num_threads
        self.session = ort.InferenceSession(self.session._model_path, options)

    def set_inter_op_num_threads(self, num_threads):
        options = ort.SessionOptions()
        options.inter_op_num_threads = num_threads
        self.session = ort.InferenceSession(self.session._model_path, options)

    def enable_memory_pattern(self):
        options = ort.SessionOptions()
        options.enable_mem_pattern = True
        self.session = ort.InferenceSession(self.session._model_path, options)

    def set_execution_mode(self, mode):
        options = ort.SessionOptions()
        options.execution_mode = mode  # ORT_SEQUENTIAL or ORT_PARALLEL
        self.session = ort.InferenceSession(self.session._model_path, options)

class PerformanceEvaluator:
    def __init__(self, cpu_model, gpu_model):
        self.cpu_model = cpu_model
        self.gpu_model = gpu_model

    def measure_inference_time(self, model, input_data, num_runs=100):
        start_time = time.time()
        for _ in range(num_runs):
            model.run(None, input_data)
        end_time = time.time()
        avg_time = (end_time - start_time) / num_runs
        return avg_time

    def calculate_accuracy(self, model, test_data, ground_truth):
        predictions = model.run(None, test_data)
        # Implement accuracy calculation based on your OCR task
        # This is a placeholder and needs to be adapted to your specific use case
        accuracy = np.mean(predictions == ground_truth)
        return accuracy

    def compare_performance(self, test_images):
        cpu_times = []
        gpu_times = []
        cpu_accuracies = []
        gpu_accuracies = []

        for image_path in test_images:
            # Load and preprocess image
            image = Image.open(image_path)
            input_data = self.preprocess_image(image)

            # Measure CPU performance
            cpu_time = self.measure_inference_time(self.cpu_model, input_data)
            cpu_accuracy = self.calculate_accuracy(self.cpu_model, input_data, ground_truth)

            # Measure GPU performance
            gpu_time = self.measure_inference_time(self.gpu_model, input_data)
            gpu_accuracy = self.calculate_accuracy(self.gpu_model, input_data, ground_truth)

            cpu_times.append(cpu_time)
            gpu_times.append(gpu_time)
            cpu_accuracies.append(cpu_accuracy)
            gpu_accuracies.append(gpu_accuracy)

        return {
            'cpu_avg_time': np.mean(cpu_times),
            'gpu_avg_time': np.mean(gpu_times),
            'cpu_avg_accuracy': np.mean(cpu_accuracies),
            'gpu_avg_accuracy': np.mean(gpu_accuracies)
        }

    def preprocess_image(self, image):
        # Implement image preprocessing
        # This is a placeholder and needs to be adapted to your specific use case
        return np.array(image)

def main():
    # Paths
    gpu_model_path = 'pathtogpu_model.pth'
    onnx_path = 'pathtomodel.onnx'
    optimized_path = 'pathtooptimized_model.onnx'
    quantized_path = 'pathtoquantized_model.onnx'

    # Convert model
    converter = ModelConverter(gpu_model_path)
    converter.convert_to_onnx(onnx_path)
    converter.optimize_onnx_model(onnx_path, optimized_path)
    converter.quantize_model(optimized_path, quantized_path)

    # Optimize for CPU
    cpu_optimizer = CPUOptimizer(quantized_path)
    cpu_optimizer.set_intra_op_num_threads(4)
    cpu_optimizer.set_inter_op_num_threads(4)
    cpu_optimizer.enable_memory_pattern()
    cpu_optimizer.set_execution_mode(ort.ExecutionMode.ORT_SEQUENTIAL)

    # Evaluate performance
    evaluator = PerformanceEvaluator(cpu_optimizer.session, gpu_model)
    test_images = ['path/to/test_image1.jpg', 'path/to/test_image2.jpg']
    results = evaluator.compare_performance(test_images)

    print("Performance Comparison:")
    print(f"CPU Average Inference Time: {results['cpu_avg_time']:.4f} seconds")
    print(f"GPU Average Inference Time: {results['gpu_avg_time']:.4f} seconds")
    print(f"CPU Average Accuracy: {results['cpu_avg_accuracy']:.2f}")
    print(f"GPU Average Accuracy: {results['gpu_avg_accuracy']:.2f}")


main()